{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('merged_newsapi_data.csv')\n",
    "\n",
    "text_data = df['Lemmatized'].fillna('')\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "doc_term_matrix = vectorizer.fit_transform(text_data)\n",
    "\n",
    "num_topics = 5  \n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(doc_term_matrix)\n",
    "\n",
    "def get_top_words(model, feature_names, n_top_words):\n",
    "    top_words = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words.append(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    return top_words\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "top_words = get_top_words(lda, feature_names, 10)\n",
    "\n",
    "topic_distributions = lda.transform(doc_term_matrix)\n",
    "df['Dominant_Topic'] = np.argmax(topic_distributions, axis=1)\n",
    "df['Topic_Probability'] = np.max(topic_distributions, axis=1)\n",
    "\n",
    "topic_keywords = pd.Series(top_words, name='Topic_Keywords')\n",
    "df = df.join(topic_keywords, on='Dominant_Topic')\n",
    "\n",
    "df.to_csv('lda.csv', index=False)\n",
    "\n",
    "print(\"Topic Keywords:\")\n",
    "for i, words in enumerate(top_words):\n",
    "    print(f\"Topic {i}: {words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def load_data():\n",
    "    df = pd.read_csv('merged_newsapi_data.csv')\n",
    "    text_data = df['Lemmatized'].fillna('')\n",
    "    return df, text_data\n",
    "\n",
    "def vectorize_text(text_data):\n",
    "    vectorizer = CountVectorizer(\n",
    "        max_df=0.95, \n",
    "        min_df=2,\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 2)\n",
    "    )\n",
    "    doc_term_matrix = vectorizer.fit_transform(text_data)\n",
    "    return vectorizer, doc_term_matrix\n",
    "\n",
    "def train_lda(doc_term_matrix, num_topics=5):\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=num_topics,\n",
    "        learning_method='online',\n",
    "        random_state=42,\n",
    "        max_iter=10\n",
    "    )\n",
    "    lda.fit(doc_term_matrix)\n",
    "    return lda\n",
    "\n",
    "def visualize_topics(lda, vectorizer):\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-15 - 1:-1]]\n",
    "        word_freq = {word: topic[i] for i, word in enumerate(feature_names)}\n",
    "        \n",
    "        plt.subplot(2, 3, topic_idx+1)\n",
    "        wordcloud = WordCloud(width=600, height=400, \n",
    "                            background_color='white').generate_from_frequencies(word_freq)\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.title(f'Topic {topic_idx}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('topic_wordclouds.png')\n",
    "    plt.close()\n",
    "\n",
    "def save_results(df, lda, doc_term_matrix):\n",
    "    topic_distributions = lda.transform(doc_term_matrix)\n",
    "    df['Dominant_Topic'] = np.argmax(topic_distributions, axis=1)\n",
    "    df['Topic_Probability'] = np.max(topic_distributions, axis=1)\n",
    "    \n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    top_words = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_words.append(\", \".join([feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]))\n",
    "    \n",
    "    df['Topic_Keywords'] = df['Dominant_Topic'].map(lambda x: top_words[x])\n",
    "    df.to_csv('lda.csv', index=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    NUM_TOPICS = 5\n",
    "    df, text_data = load_data()\n",
    "    vectorizer, doc_term_matrix = vectorize_text(text_data)\n",
    "    lda = train_lda(doc_term_matrix, NUM_TOPICS)\n",
    "    visualize_topics(lda, vectorizer)\n",
    "    final_df = save_results(df, lda, doc_term_matrix)\n",
    "    \n",
    "    topic_counts = final_df['Dominant_Topic'].value_counts()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    topic_counts.sort_index().plot(kind='bar')\n",
    "    plt.title('Distribution of Topics Across Documents')\n",
    "    plt.xlabel('Topic Number')\n",
    "    plt.ylabel('Number of Documents')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.savefig('topic_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Analysis complete! Created:\")\n",
    "    print(\"- lda.csv (topic assignments)\")\n",
    "    print(\"- topic_wordclouds.png\")\n",
    "    print(\"- topic_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def load_data():\n",
    "    df = pd.read_csv('merged_newsapi_data.csv')\n",
    "    text_data = df['Lemmatized'].fillna('')\n",
    "    return df, text_data\n",
    "\n",
    "def vectorize_text(text_data):\n",
    "    vectorizer = CountVectorizer(\n",
    "        max_df=0.95, \n",
    "        min_df=2,\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 1)  \n",
    "    )\n",
    "    doc_term_matrix = vectorizer.fit_transform(text_data)\n",
    "    return vectorizer, doc_term_matrix\n",
    "\n",
    "def train_lda(doc_term_matrix, num_topics=6):  \n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=num_topics,\n",
    "        learning_method='online',\n",
    "        random_state=42,\n",
    "        max_iter=20  \n",
    "    )\n",
    "    lda.fit(doc_term_matrix)\n",
    "    return lda\n",
    "\n",
    "def generate_topic_word_table(lda, vectorizer, top_n=12):\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topic_word_table = []\n",
    "    \n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-top_n - 1:-1]]\n",
    "        topic_word_table.append(top_words)\n",
    "    \n",
    "    df_table = pd.DataFrame(topic_word_table).T\n",
    "    df_table.columns = [f\"Topic {i}\" for i in range(len(topic_word_table))]\n",
    "    \n",
    "    return df_table\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df, text_data = load_data()\n",
    "    vectorizer, doc_term_matrix = vectorize_text(text_data)\n",
    "    \n",
    "    # Train LDA model\n",
    "    lda = train_lda(doc_term_matrix)\n",
    "    \n",
    "    topic_table = generate_topic_word_table(lda, vectorizer)\n",
    "    \n",
    "    topic_table.to_csv('topic_word_table.csv', index=False)\n",
    "    print(\"Topic-Word Distribution Table:\")\n",
    "    print(topic_table.to_markdown(index=False, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def load_data():\n",
    "    df = pd.read_csv('merged_newsapi_data.csv')\n",
    "    text_data = df['Lemmatized'].fillna('')\n",
    "    return df, text_data\n",
    "\n",
    "def vectorize_text(text_data):\n",
    "    vectorizer = CountVectorizer(\n",
    "        max_df=0.95, \n",
    "        min_df=2,\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 1)\n",
    "    )\n",
    "    return vectorizer, vectorizer.fit_transform(text_data)\n",
    "\n",
    "def train_lda(doc_term_matrix, num_topics=6):\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=num_topics,\n",
    "        learning_method='online',\n",
    "        random_state=42,\n",
    "        max_iter=20\n",
    "    )\n",
    "    return lda.fit(doc_term_matrix)\n",
    "\n",
    "def generate_topic_table(lda, vectorizer, top_n=12):\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topic_words = []\n",
    "    \n",
    "    for topic in lda.components_:\n",
    "        top_words_idx = topic.argsort()[:-top_n - 1:-1]\n",
    "        topic_words.append([feature_names[i] for i in top_words_idx])\n",
    "    \n",
    "    # Create markdown table\n",
    "    table = \"| \" + \" | \".join([f\"Topic {i}\" for i in range(len(topic_words))]) + \" |\\n\"\n",
    "    table += \"|\" + \"|\".join([\"---\"] * len(topic_words)) + \"|\\n\"\n",
    "    \n",
    "    for i in range(len(topic_words[0])):\n",
    "        table += \"| \" + \" | \".join([topic_words[t][i] for t in range(len(topic_words))]) + \" |\\n\"\n",
    "    \n",
    "    with open('topic_word_table.txt', 'w') as f:\n",
    "        f.write(table)\n",
    "    \n",
    "    return topic_words\n",
    "\n",
    "def create_visualizations(lda, vectorizer, df):\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    plt.figure(figsize=(20, 12))\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        word_freq = {feature_names[i]: topic[i] for i in topic.argsort()[:-20 - 1:-1]}\n",
    "        \n",
    "        plt.subplot(2, 3, topic_idx+1)\n",
    "        wordcloud = WordCloud(width=800, height=600, \n",
    "                            background_color='white').generate_from_frequencies(word_freq)\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.title(f'Topic {topic_idx}', fontsize=16)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('topic_wordclouds.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    topic_dist = np.argmax(lda.transform(doc_term_matrix), axis=1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    pd.Series(topic_dist).value_counts().sort_index().plot(kind='bar')\n",
    "    plt.title('Document Distribution Across Topics', fontsize=14)\n",
    "    plt.xlabel('Topic Number', fontsize=12)\n",
    "    plt.ylabel('Number of Documents', fontsize=12)\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.savefig('topic_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df, text_data = load_data()\n",
    "    vectorizer, doc_term_matrix = vectorize_text(text_data)\n",
    "    \n",
    "    lda = train_lda(doc_term_matrix)\n",
    "    \n",
    "    topic_words = generate_topic_table(lda, vectorizer)\n",
    "    create_visualizations(lda, vectorizer, df)\n",
    "    \n",
    "    pd.DataFrame(topic_words).T.to_csv(\n",
    "        'topic_word_table.csv',\n",
    "        header=[f\"Topic {i}\" for i in range(len(topic_words))],\n",
    "        index=False\n",
    "    )\n",
    "    \n",
    "    print(\"Successfully created:\")\n",
    "    print(\"- topic_word_table.txt (text format)\")\n",
    "    print(\"- topic_word_table.csv (spreadsheet format)\")\n",
    "    print(\"- topic_wordclouds.png (visualization)\")\n",
    "    print(\"- topic_distribution.png (chart)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.manifold import MDS\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "def load_data():\n",
    "    df = pd.read_csv('merged_newsapi_data.csv')\n",
    "    text_data = df['Lemmatized'].fillna('')\n",
    "    return df, text_data\n",
    "\n",
    "def vectorize_text(text_data):\n",
    "    vectorizer = CountVectorizer(\n",
    "        max_df=0.95, \n",
    "        min_df=2,\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 2) \n",
    "    )\n",
    "    return vectorizer, vectorizer.fit_transform(text_data)\n",
    "\n",
    "def train_lda(doc_term_matrix, num_topics=6):\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=num_topics,\n",
    "        learning_method='online',\n",
    "        random_state=42,\n",
    "        max_iter=20\n",
    "    )\n",
    "    return lda.fit(doc_term_matrix)\n",
    "\n",
    "def generate_topic_table(lda, vectorizer, top_n=12):\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topic_words = []\n",
    "    \n",
    "    for topic in lda.components_:\n",
    "        top_words_idx = topic.argsort()[:-top_n - 1:-1]\n",
    "        topic_words.append([feature_names[i] for i in top_words_idx])\n",
    "    \n",
    "    with open('topic_word_table.txt', 'w') as f:\n",
    "        f.write(\"| \" + \" | \".join([f\"Topic {i}\" for i in range(len(topic_words))]) + \" |\\n\")\n",
    "        f.write(\"|\" + \"|\".join([\"---\"] * len(topic_words)) + \"|\\n\")\n",
    "        for i in range(len(topic_words[0])):\n",
    "            f.write(\"| \" + \" | \".join([topic_words[t][i] for t in range(len(topic_words))]) + \" |\\n\")\n",
    "    \n",
    "    return topic_words\n",
    "\n",
    "def create_intertopic_map(lda):\n",
    "    topic_dist = pdist(lda.components_, metric='cosine')\n",
    "    dist_matrix = squareform(topic_dist)\n",
    "    \n",
    "    mds = MDS(n_components=2, dissimilarity='precomputed', random_state=42)\n",
    "    pos = mds.fit_transform(dist_matrix)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i in range(len(pos)):\n",
    "        plt.scatter(pos[i, 0], pos[i, 1], s=200)\n",
    "        plt.text(pos[i, 0]+0.02, pos[i, 1]+0.02, f'Topic {i}', \n",
    "                fontsize=12, ha='center', va='center')\n",
    "    \n",
    "    plt.title('Intertopic Distance Map (MDS)', fontsize=16)\n",
    "    plt.xlabel('Dimension 1', fontsize=14)\n",
    "    plt.ylabel('Dimension 2', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('intertopic_distance_map.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def create_visualizations(lda, vectorizer):\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    plt.figure(figsize=(20, 12))\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        word_freq = {feature_names[i]: topic[i] for i in topic.argsort()[:-20 - 1:-1]}\n",
    "        \n",
    "        plt.subplot(2, 3, topic_idx+1)\n",
    "        wordcloud = WordCloud(width=800, height=600, \n",
    "                            background_color='white').generate_from_frequencies(word_freq)\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.title(f'Topic {topic_idx}', fontsize=16)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('topic_wordclouds.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    pd.Series(np.argmax(lda.transform(doc_term_matrix), axis=1)).value_counts().sort_index().plot(kind='bar')\n",
    "    plt.title('Document Distribution Across Topics', fontsize=14)\n",
    "    plt.xlabel('Topic Number', fontsize=12)\n",
    "    plt.ylabel('Number of Documents', fontsize=12)\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.savefig('topic_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df, text_data = load_data()\n",
    "    vectorizer, doc_term_matrix = vectorize_text(text_data)\n",
    "    \n",
    "    lda = train_lda(doc_term_matrix)\n",
    "    \n",
    "    topic_words = generate_topic_table(lda, vectorizer)\n",
    "    create_visualizations(lda, vectorizer)\n",
    "    create_intertopic_map(lda)\n",
    "    \n",
    "    pd.DataFrame(topic_words).T.to_csv(\n",
    "        'topic_word_table.csv',\n",
    "        header=[f\"Topic {i}\" for i in range(len(topic_words))],\n",
    "        index=False\n",
    "    )\n",
    "    \n",
    "    print(\"Successfully created:\")\n",
    "    print(\"- topic_word_table.txt/.csv\")\n",
    "    print(\"- topic_wordclouds.png\")\n",
    "    print(\"- topic_distribution.png\")\n",
    "    print(\"- intertopic_distance_map.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df = pd.read_csv('merged_newsapi_data.csv')\n",
    "\n",
    "texts = df['Lemmatized'].fillna('')  \n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000)  \n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "result_df = pd.concat([df, tfidf_df], axis=1)\n",
    "\n",
    "result_df.to_csv('tfidf.csv', index=False)\n",
    "\n",
    "print(\"TF-IDF transformation completed and saved to 'tfidf.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
